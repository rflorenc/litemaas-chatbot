# ğŸ‰ Open Source Mentor Bot - WORKING!

**Status**: âœ… Fully Operational
**Last Updated**: 2025-11-03
**URL**: http://localhost:8080

## âœ… Working Components

### 1. Container
- **Status**: Running and healthy
- **Runtime**: Podman with podman-compose
- **Base Image**: python:3.11-slim
- **Resources**: 256MB-512MB RAM, 0.5-1.0 CPU

### 2. Web Application
- **Framework**: Flask + Gunicorn
- **Workers**: 2 workers with 4 threads each
- **Port**: 8080
- **Health Endpoint**: http://localhost:8080/health âœ…

### 3. LiteLLM Integration
- **Provider**: LiteLLM (DeepSeek-R1-Distill-Qwen-14B-W4A16)
- **Endpoint**: https://litellm-litemaas.apps.prod.rhoai.rh-aiservices-bu.com/v1/chat/completions
- **API Key**: Configured and working âœ…
- **Response Format**: Handles both `content` and `reasoning_content`

### 4. Web UI
- **Accessible**: http://localhost:8080/
- **Features**:
  - Beautiful gradient interface
  - Real-time chat functionality
  - Mobile-responsive design
  - Red Hat branding

### 5. API Endpoints

#### GET /
Web UI interface âœ…

#### GET /health
```json
{
    "service": "open-source-mentor-bot",
    "status": "healthy",
    "version": "1.0.0"
}
```

#### POST /api/chat
```bash
curl -X POST http://localhost:8080/api/chat \
  -H 'Content-Type: application/json' \
  -d '{"message": "What is open source?"}'
```

**Response**: âœ… Working - Returns detailed explanations about open source, Red Hat values, and community collaboration

## ğŸ§ª Test Results

### Chat API Test
```json
{
    "response": "Alright, so I'm trying to understand what open source is...",
    "status": "success"
}
```
âœ… **PASS** - LLM responds with thoughtful, educational answers

### Health Check
```json
{
    "service": "open-source-mentor-bot",
    "status": "healthy",
    "version": "1.0.0"
}
```
âœ… **PASS** - Application is healthy

### Container Status
```
rlteam-mentor-bot	Up About a minute (healthy)	0.0.0.0:8080->8080/tcp
```
âœ… **PASS** - Container running and healthy

## ğŸš€ Quick Start Commands

```bash
# Start the application
make up

# View logs
make logs

# Stop the application
make down

# Restart
make restart

# Check health
make health

# Open shell in container
make shell

# Run tests
make test
```

## ğŸ“ Configuration

### Environment Variables (.env)
```bash
PORT=8080
FLASK_ENV=production
LITEMAAS_BASE_URL=https://litellm-litemaas.apps.prod.rhoai.rh-aiservices-bu.com
LITEMAAS_API_KEY=REPLACE_ME
COMPOSE_PROJECT_NAME=rlteam-mentorbot
PYTHONUNBUFFERED=1
```

### Model Configuration
- **Model**: DeepSeek-R1-Distill-Qwen-14B-W4A16
- **Max Tokens**: 500 (configurable)
- **Temperature**: 0.7
- **Top P**: 0.9

## ğŸ”’ Security Features

âœ… Input sanitization (HTML escaping, length limits)
âœ… Prompt injection protection
âœ… Non-root container user (UID 1001)
âœ… Environment-based API key storage
âœ… Request validation
âœ… Production WSGI server (Gunicorn)

## ğŸ“Š Performance

- **Startup Time**: ~5 seconds
- **Memory Usage**: ~256MB typical
- **Response Time**: 1-3 seconds (depends on LLM)
- **Container Size**: ~400MB

## ğŸ¯ Educational Focus

The bot teaches about:
- âœ… Open source best practices
- âœ… Red Hat core values (Open Collaboration, Transparency, Community First, Automation, Trust)
- âœ… Getting started with contributions
- âœ… Containerization with Podman
- âœ… Community collaboration

## ğŸ“ Project Structure

```
rlteam/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py           âœ… Package init
â”‚   â”œâ”€â”€ main.py               âœ… Flask app + Web UI
â”‚   â”œâ”€â”€ litemaas_client.py    âœ… LLM integration
â”‚   â””â”€â”€ utils.py              âœ… Input validation
â”œâ”€â”€ openshift/                      âœ… Kubernetes manifests
â”œâ”€â”€ tests/                    âœ… Test suite
â”œâ”€â”€ Containerfile             âœ… Container build
â”œâ”€â”€ compose.yaml              âœ… Podman Compose config
â”œâ”€â”€ Makefile                  âœ… Automation
â”œâ”€â”€ requirements.txt          âœ… Dependencies
â”œâ”€â”€ .env                      âœ… Environment config
â””â”€â”€ README.md                 âœ… Documentation
```

## ğŸ› Troubleshooting

### If the bot isn't responding:
1. Check logs: `make logs`
2. Verify API key is correct in `.env`
3. Check LiteLLM endpoint is accessible
4. Restart: `make restart`

### If port 8080 is in use:
1. Edit `.env` and change `PORT=8081`
2. Restart: `make restart`

### If container won't start:
1. Check logs: `podman logs rlteam-mentor-bot`
2. Verify .env file exists: `ls -la .env`
3. Rebuild: `make clean && make build-run`

## ğŸ“ Next Steps

1. **Try the Web UI**: Open http://localhost:8080 in your browser
2. **Ask Questions**: Test the chatbot with various open source questions
3. **Customize**: Modify the system prompt in `app/litemaas_client.py`
4. **Deploy**: Use the k8s manifests for Kubernetes/OpenShift deployment

## âœ… Ready for Demo!

The application is fully functional and ready to demonstrate:
- âœ… Containerized architecture with Podman
- âœ… LLM integration with LiteLLM
- âœ… Beautiful web interface
- âœ… RESTful API
- âœ… Health monitoring
- âœ… Production-ready security
- âœ… Red Hat values integration

---

**Built with â¤ï¸ for the Red Hat Open Source Hackathon**
